# -*- coding: utf-8 -*-
"""Gold Price Prediction .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10VMq7Iz7v0crQDeuDVje1Op57sWF3Jrn

# **Long Short Term Memory (LSTM)**
"""

# Import Libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_percentage_error
import tensorflow as tf
from keras import Model
from keras.layers import Input, Dense, Dropout
from keras.layers import LSTM

"""# Data Collection"""

df = pd.read_csv('/content/drive/MyDrive/Gold Price prediction/goldstock.csv')

"""# Data Overview"""

df.head()

df.tail()

'''from matplotlib import pyplot as plt
_df_14['Close'].plot(kind='line', figsize=(8, 4), title='Close')
plt.gca().spines[['top', 'right']].set_visible(False)'''

'''from matplotlib import pyplot as plt
_df_1['Close'].plot(kind='hist', bins=20, title='Close')
plt.gca().spines[['top', 'right',]].set_visible(False)'''

'''from matplotlib import pyplot as plt
_df_2['Volume'].plot(kind='hist', bins=20, title='Volume')
plt.gca().spines[['top', 'right',]].set_visible(False)'''

df.shape

df.info()

"""# Data Pre Processing

##Feature Subset Selection¶
Since we will not use Volume and Unnammed: 0 features to predict Price, we will drop these two features:
"""

df.drop(['Volume', 'Unnamed: 0'], axis=1, inplace=True)

"""## Transforming Data
Date feature is stored as object in the data frame. To increase the speed of calculations, we convert it's data type to datetime and then sort this feature in ascending order:
"""

df['Date'] = pd.to_datetime(df['Date'])
df.sort_values(by='Date', ascending=True, inplace=True)
df.reset_index(drop=True, inplace=True)

"""Since, 'Close' is final (Price) of each day"""

df.rename(columns={'Close': 'Price'}, inplace=True)

df.head()

# @title Open, High, Low, and Close Prices

import matplotlib.pyplot as plt
plt.plot(df['Date'], df['Open'], label='Open')
plt.plot(df['Date'], df['High'], label='High')
plt.plot(df['Date'], df['Low'], label='Low')
plt.plot(df['Date'], df['Price'], label='Close')
plt.xlabel('Date')
plt.ylabel('Price')
_ = plt.legend()

from matplotlib import pyplot as plt
import seaborn as sns
def _plot_series(series, series_name, series_index=0):
  from matplotlib import pyplot as plt
  import seaborn as sns
  palette = list(sns.palettes.mpl_palette('Dark2'))
  xs = series['Date']
  ys = series['Price']

  plt.plot(xs, ys, label=series_name, color=palette[series_index % len(palette)])

fig, ax = plt.subplots(figsize=(10, 5.2), layout='constrained')
df_sorted = _df_65.sort_values('Date', ascending=True)
_plot_series(df_sorted, '')
sns.despine(fig=fig, ax=ax)
plt.xlabel('Date')
_ = plt.ylabel('Price')

"""## Checking missing values"""

df.isnull().sum()

"""# Data Analysis

Getting Statistical measures of data
"""

df.describe()

"""## Correlations:
### 1. Positive Correlations
### 2. Negative Correlations
"""

correlation = df.corr()

"""Construcing Heatmap to understand correlations."""

plt.figure(figsize=(8,8))
sns.heatmap(correlation, cbar=True, square=True, fmt='.1f', annot=True, annot_kws={'size':9}, cmap='Blues')
plt.show()

print(correlation['Price'])

# Check the distribution of Price
sns.displot(df['Price'], color='green')

"""## Calculate and plot rolling statistics"""

rolling_mean = df.rolling(window=12).mean()
rolling_std = df.rolling(window=12).std()

print(rolling_mean)
print(rolling_std)

plt.figure(figsize=(8, 4))
plt.plot(rolling_mean, label='Rolling Mean')
plt.plot(rolling_std, label='Rolling Std')
plt.title('Rolling Statistics')
plt.legend()
plt.show()

"""## Perform Augmented Dickey-Fuller (ADF) test"""

from statsmodels.tsa.stattools import adfuller

adf_result = adfuller(df['Price'])

print('ADF Statistic:', adf_result[0])
print('p-value:', adf_result[1])
print('Critical Values:')
for key, value in adf_result[4].items():
    print(f'{key}: {value}')

if adf_result[1] < 0.05:
    print('Reject the null hypothesis. The data is stationary.')
else:
    print('Fail to reject the null hypothesis. The data is non-stationary.')

"""## Visualizing Gold Price History Data
Interactive Gold Price Chart:
"""

fig = px.line(y=df.Price, x=df.Date)
fig.update_traces(line_color='black')
fig.update_layout(xaxis_title="Date",
                  yaxis_title="Scaled Price",
                  title={'text': "Gold Price History Data", 'y':0.95, 'x':0.5, 'xanchor':'center', 'yanchor':'top'},
                  plot_bgcolor='rgba(255,223,0,0.8)')

"""# Splitting Data to Training & Test Sets¶
Since we cannot train on future data in time series data, we should not divide the time series data randomly. In time series splitting, testing set is always later than training set. We consider the last year for testing and everything else for training:
"""

test_size_2023 = df[df.Date.dt.year == 2023].shape[0]
test_size_2024 = df[df.Date.dt.year == 2024].shape[0]

test_size = test_size_2023 + test_size_2024
print(test_size)

"""Gold Price Training and Test Sets Plot:"""

# Use Index Slicing in 'Data' and 'Price' cloumn
plt.figure(figsize=(15, 6), dpi=150)
plt.rcParams['axes.facecolor'] = 'yellow'
plt.rc('axes',edgecolor='white')
plt.plot(df.Date[:-test_size], df.Price[:-test_size], color='black', lw=2)
plt.plot(df.Date[-test_size:], df.Price[-test_size:], color='blue', lw=2)
plt.title('Gold Price Training and Test Sets', fontsize=15)
plt.xlabel('Date', fontsize=12)
plt.ylabel('Price', fontsize=12)
plt.legend(['Training set', 'Test set'], loc='upper left', prop={'size': 15})
plt.grid(color='white')
plt.show()

"""# Data Scaling
Since we aim to predict Price only based on its historical data, we scale Price using MinMaxScaler to avoid intensive computations:
"""

scaler = MinMaxScaler()
scaler.fit(df.Price.values.reshape(-1,1))

"""# Restructure Data & Create Sliding Window
The use of prior time steps to predict the next time step is called sliding window. In this way, time series data can be expressed as supervised learning. We can do this by using previous time steps as input variables and use the next time step as the output variable. The number of previous time steps is called the window width. Here we set window width to 60. Therefore, X_train and X_test will be nested lists containing lists of 60 time-stamp prices. y_train and y_test are also lists of gold prices containing the next day's gold price corresponds to each list in X_train and X_test respectively:
"""

window_size = 60

"""Training set:"""

train_data = df.Price[:-test_size]
train_data = scaler.transform(train_data.values.reshape(-1,1))

print(train_data.shape)

X_train = []
y_train = []

for i in range(window_size, len(train_data)):
    X_train.append(train_data[i-60:i, 0])
    y_train.append(train_data[i, 0])

"""Test set:"""

test_data = df.Price[-test_size-60:]
test_data = scaler.transform(test_data.values.reshape(-1,1))

X_test = []
y_test = []

for i in range(window_size, len(test_data)):
    X_test.append(test_data[i-60:i, 0])
    y_test.append(test_data[i, 0])

"""# Converting Data to Numpy Arrays
Now X_train and X_test are nested lists (two-dimensional lists) and y_train is a one-dimensional list. We need to convert them to numpy arrays with a higher dimension, which is the data format accepted by TensorFlow when training the neural network:
"""

X_train = np.array(X_train)
X_test  = np.array(X_test)
y_train = np.array(y_train)
y_test  = np.array(y_test)

X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))
X_test  = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))
y_train = np.reshape(y_train, (-1,1))
y_test  = np.reshape(y_test, (-1,1))

print('X_train Shape: ', X_train.shape)
print('y_train Shape: ', y_train.shape)
print('X_test Shape:  ', X_test.shape)
print('y_test Shape:  ', y_test.shape)

"""# Creating an LSTM Network
We build an LSTM network, which is a type of Recurrent Neural Networks designed to solve vanishing gradient problem:

Model Definition:
"""

def define_model():
    input1 = Input(shape=(window_size,1))
    x = LSTM(units = 64, return_sequences=True)(input1)
    x = Dropout(0.2)(x)
    x = LSTM(units = 64, return_sequences=True)(x)
    x = Dropout(0.2)(x)
    x = LSTM(units = 64)(x)
    x = Dropout(0.2)(x)
    x = Dense(32, activation='softmax')(x)
    dnn_output = Dense(1)(x)

    model = Model(inputs=input1, outputs=[dnn_output])
    model.compile(loss='mean_squared_error', optimizer='Nadam')
    model.summary()

    return model

"""Model Training:"""

model = define_model()
history = model.fit(X_train, y_train, epochs=150, batch_size=32, validation_split=0.1, verbose=1)

"""# Model Evaluation¶
Next, we evaluate our time series forecast using MAPE (Mean Absolute Percentage Error) metric:
"""

result = model.evaluate(X_test, y_test)
y_pred = model.predict(X_test)

MAPE = mean_absolute_percentage_error(y_test, y_pred)
Accuracy = 1 - MAPE

print("Test Loss:", result)
print("Test MAPE:", MAPE)
print("Test Accuracy:", Accuracy)

"""# Visualizing Results
Returning the actual and predicted Price values to their primary scale:
"""

y_test_true = scaler.inverse_transform(y_test)
y_test_pred = scaler.inverse_transform(y_pred)

"""Investigating the closeness of the prices predicted by the model to the actual prices:"""

plt.figure(figsize=(15, 6), dpi=150)
plt.rcParams['axes.facecolor'] = 'yellow'
plt.rc('axes',edgecolor='white')
plt.plot(df['Date'].iloc[:-test_size], scaler.inverse_transform(train_data), color='black', lw=2)
plt.plot(df['Date'].iloc[-test_size:], y_test_true, color='blue', lw=2)
plt.plot(df['Date'].iloc[-test_size:], y_test_pred, color='red', lw=2)
plt.title('Model Performance on Gold Price Prediction', fontsize=15)
plt.xlabel('Date', fontsize=12)
plt.ylabel('Price', fontsize=12)
plt.legend(['Training Data', 'Actual Test Data', 'Predicted Test Data'], loc='upper left', prop={'size': 15})
plt.grid(color='white')
plt.show()

